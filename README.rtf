{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c99985;}
\paperw11900\paperh16840\margl1440\margr1440\vieww20020\viewh13860\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\sl264\slmult1\pardirnatural\partightenfactor0

\f0\fs24 \cf2 \cb3 # BLIP Image Captioning (IBM Labs)\
\
Mini-progetto in Python basato sul modello `Salesforce/blip-image-captioning-base` di Hugging Face.  \
Gli esercizi mostrano un'evoluzione in tre step: script base da terminale, interfaccia web con Gradio e generazione automatica di didascalie da URL. [web:386][web:507]\
\
---\
\
## Requisiti\
\
- Python 3.10+\
- Ambiente virtuale (consigliato)\
- Librerie principali:\
  - `transformers`\
  - `torch`\
  - `pillow`\
  - `requests`\
  - `gradio` (per la UI web) [web:490][web:489]\
\
Installa le dipendenze con:\
\
```bash\
pip install -r requirements.txt\
\
\
Step 1 \'96 Script base da terminale\
File:\'a0image_cap.py\
Carica il modello BLIP e genera una didascalia per un'immagine locale.\
Uso tipico:\
Metti un'immagine nella cartella del progetto (es.\'a0CRYPTOEMDAY.png).\
Imposta il nome del file immagine nel codice (img_path).\
Esegui:\
python image_cap.py\
Lo script stampa in console la didascalia generata dal modello\
\
\
Step 2 \'96 Interfaccia web con Gradio\
File:\
hello.py\'a0\'96 esempio minimo per testare che Gradio funzioni.\
image_captioning_app.py\'a0\'96 applicazione completa di image captioning con interfaccia. [web:515][web:490]\
Esempio di esecuzione della UI principale:\
\
python image_captioning_app.py\
\
Funzionalit\'e0:\
Apertura di una semplice interfaccia web locale.\
Upload di un'immagine dal browser.\
Generazione e visualizzazione della didascalia in tempo reale. [web:489][web:515]\
\
\
Step 3 \'96 URL captioner (automazione da siti)\
File:\
automate_url_captioner.py\
captions.txt\'a0(output di esempio)\
Questo script:\
Legge una lista di URL di immagini da un sito predefinito.\
Scarica le immagini.\
Genera automaticamente le didascalie con BLIP.\
Salva i risultati nel file\'a0captions.txt. [web:489][web:517]\
Esecuzione (sezione URL impostata nel codice):\
\
\
python automate_url_captioner.py\
\
\
Il file\'a0captions.txt\'a0mostra un esempio reale delle descrizioni prodotte per le immagini del sito analizzato.\
\
\
Note sul progetto\
Tutti gli step condividono lo stesso modello BLIP (Salesforce/blip-image-captioning-base) ma con modalit\'e0 d'uso diverse: CLI, UI web, automazione da URL. [web:386][web:520]\
Il progetto \'e8 stato sviluppato come parte di esercizi IBM e pu\'f2 essere esteso con:\
interfacce pi\'f9 curate,\
supporto a input multipli,\
integrazione con altri servizi o dataset.\
}